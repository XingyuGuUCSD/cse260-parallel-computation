\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{framed} % or, "mdframed"
\usepackage[framed]{ntheorem}
\usepackage{listings}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\newframedtheorem{frm-thm}{Lemma}

\usepackage[scale=1.0, left=1.0cm, right=1.0cm, top=1.865cm, bottom=0.865cm]{geometry}
\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{dred}{rgb}{0.545,0,0}
\definecolor{dblue}{rgb}{0,0,0.545}
\definecolor{lgrey}{rgb}{0.9,0.9,0.9}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\lstset{ 
      backgroundcolor=\color{lgrey},  
      basicstyle=\footnotesize \ttfamily \color{black} \bfseries,   
      breakatwhitespace=false,       
      breaklines=true,               
      captionpos=b,                   
      commentstyle=\color{dkgreen},   
      deletekeywords={...},          
      escapeinside={\%*}{*)},                  
      frame=single,                  
      keywordstyle=\color{purple},  
      morekeywords={BRIEFDescriptorConfig,string,TiXmlNode,DetectorDescriptorConfigContainer,istringstream,cerr,exit}, 
      identifierstyle=\color{black},
      stringstyle=\color{blue},      
      language=C,                
      numbers=right,                 
      numbersep=5pt,                  
      numberstyle=\tiny\color{black}, 
      rulecolor=\color{black},        
      showspaces=false,               
      showstringspaces=false,        
      showtabs=false,                
      stepnumber=1,                   
      tabsize=5,                     
      title=\lstname,                 
    }


\pagestyle{myheadings}
\markright{CSE 260, Assignment 1 \hfill Andrew Conegliano, Matthias Springer\hfill}

\begin{document}

\title{Double-precision General Matrix Multiply (DGEMM)  \\ \vspace{2 mm} {\large Parallel Computation (CSE 260), Assignment 1}}
%\subtitle{Parallel Computation (CSE 260), Assignment 1}
\date{\today}
\author{Andrew Conegliano \and Matthias Springer}
\maketitle

\section{Introduction}

\subsection{Assumptions}
\begin{itemize}
	\item All matrices are square matrices.
	\item All matrices consist of double-precision floating point values (64-bit doubles).
\end{itemize}

\subsection{Notation}
In this work, we use the following notation and variable names.
\begin{itemize}
	\item $n$ is the size of one dimension of the matrices involved. I.e., every matrix has $n^2$ values.
	\item When referring to matrices, $A$ and $B$ denote the source matrices and $C$ denotes the target matrix, i.e. $AB = C$.
	\item $b_i$, $b_j$ and $b_k$ denote the block size for each \emph{dimension} $i$, $j$, and $k$, respectively\footnote{We tried multiple levels of blocking and it is evident from the context which level of blocking we are refering to.}.
\end{itemize}

\section{Runtime Environment}
We optimized our DGEMM implementation for a specific runtime environment. All benchmarks and performance results are based on the following hardware and software.

\subsection{Hardware}
\begin{figure}
	\begin{center}
	\includegraphics[width=0.5\textwidth]{xeon_die.pdf}
	\end{center}
	\caption{Architecture of the CPU}
\end{figure}

\begin{itemize}
	\item Intel Xeon E5354 @ 2.33GHz (\emph{Clovertown} processor)
	\begin{itemize}
		\item 2 \emph{Woodcrest} Core2 dies
		\item 2 sockets per chip
		\item Supports SSE, SSE2, SSSE3
	\end{itemize}
	\item Memory hierarchy
	\begin{itemize}
		\item $32$ KB Level 1 cache
		\item $4$ MB shared Level 2 cache
		\item $64$ byte cache line
		\item $16 \times 128$ bit SSE3 registers
	\end{itemize}
\end{itemize}

\subsection{Software}
\begin{itemize}
	\item CentOS 6.3 Linux (Kernel \texttt{2.6.32-358.18.1.el6.x86\_64})
	\item Built with GCC 4.7.3
	\item Compiler flags: \texttt{-foptimize-register-move -O3 -msse -funroll-loops -msse2 -msse3 -m3dnow -mfpmath=sse -mfpmath=sse -malign-double}
\end{itemize}

\section{Basic Algorithm}
The naive implementation of the matrix multiplication algorithm consists of three nested \lstinline{for} loops, where every loop runs from $1$ to $n$. In the innermost loop, a single addition and multiplication is done. Therefore, the runtime complexity of this algorithm is $\mathcal{O}(n^3)$ floating-point operations. We did not use an algorithm with a lower runtime complexity\footnote{Strassen's algorithm has a runtime complexity of $\mathcal{O}(n^{2.8})$.}.

\begin{figure}
\begin{lstlisting}
void square_dgemm (int n, double* A, double* B, double* C)
{
 	for (int i = 0; i < n; ++i)
    {
    	for (int j = 0; j < n; ++j)
    	{
			double cij = C[i*n+j];

			for (int k = 0; k < n; k++)
			{
				cij += A[i*n+k] * B[k*n+j];
				C[i*n+j] = cij;
    		}
    	}
    }
}
\end{lstlisting}
\caption{Naive matrix multiplication algorithm.}
\label{fig:naive_mul}
\end{figure}
Figure~\ref{fig:naive_mul} shows the basic matrix multiplication algorithm. We will reference this algorithm throughout the Section~\ref{sec:optim} and explain why our optimizations can increase the performance of the basic algorithm.

\section{Optimizations}\label{sec:optim}
In this section, we will describe and evaluation optimizations of our DGEMM algorithm.

\subsection{Blocking for L1 Cache}
To increase locality, we implemented blocking.  This restricts the computations into chunks that that fit inside the cache. In the basic algorithm, we read \lstinline{A} row by row left-to-right. \lstinline{B}, on the other hand, is read column by column from the top to the bottom. Every time we access a value in a matrix, we prefetch the next $7$ elements in that row\footnote{A cache line contains $8$ doubles.}, in case we have a cache miss.

%Main memory references are also decreased from $n^3+3n^2$ references in the naive implementation to $2*(n+2)*n^2$. The flop to memory ratio, $q$, is also drastically reduced from $q=2$ as n $\rightarrow \infty$ to $q=n/N=\mathit{BLOCK\_SIZE}$ as n $\rightarrow \infty$.  As the flop to memory ratio decreases, so does the number of caches misses. There are $(n^2_B/8 + n^2_B/8)(n/\mathit{BLOCK\_SIZE})$ cache misses for every C block computed, multiplied by $(N/\mathit{BLOCK\_SIZE})^2$ blocks in C, for a total of $N^3/(4N_B)$. , compared to $(9/8)N^3$.

\begin{lstlisting}
for (int i = 0; i < BLOCK_SIZE; i++)
	for (int j = 0; j < BLOCK_SIZE; j++)
		for (int k = 0; k < BLOCK_SIZE; k++)
			C[i][j]+=A[i][k]+B[k][j];
\end{lstlisting}

%Handling fringe cases

\subsection{Blocking for L2 Cache}
The size of the Level 2 cache is $4$ MB. Therefore, we can store $\frac{4 \cdot 1024^2}{8}$ matrix entries in the L2 cache. Since we have three matrices \lstinline{A}, \lstinline{B}, and \lstinline{C}, blocking for the L2 cache can increase the performance only if $n > \sqrt{\frac{4 \cdot 1024^2}{8 \cdot 3}} = 418$. In our benchmarks, we did not deal with matrices bigger than $1200$ entries. Therefore, we do no blocking for the L2 cache in our final implementation. We implemented L2 blocking in an intermediate version and it lead to a performance decrease, especially for small matrix sizes.
% theoretical explanation why it does not work

\subsection{Matrix Transposition}
To take advantage of cache locality, we transpose the matrix \lstinline{B} before running the actual algorithm. In the basic version of the algorithm, we access \lstinline{B} column by column. For sufficiently big matrices, every subsequent access into \lstinline{B} triggers a cache miss and loads a new cache line into the L1 cache. The L1 cache is $32$ KB big, resulting in $\frac{32 \cdot 1024}{64 \cdot 3} = 170$ cache lines per matrix. When we start accessing the $171$th row, the first row is evicted from the cache\footnote{Assuming a LRU replacement strategy.}. This problem is solved by L1 blocking as long as the block size is smaller or equal to $170$. For bigger block sizes, transposing the matrix helps, because when reading the matrix row by row, we have a lot of cache hits after every cache miss due to prefetching. 

\subsection{Register Blocking and Loop Unrolling}

\subsection{Matrix Buffering}
Matrix buffering is the process of duplicating the whole matrix or a part of the matrix in the memory. \lstinline{B} is buffered entirely before the run of the actual algorithm, as part of the matrix transposition. \lstinline{A} is buffered partly: when blocking for the L1 cache, we buffer \lstinline{A} block by block, i.e. we only buffer the parts of \lstinline{A} that are contained the current block of \lstinline{A}. We tried buffering the matrix \lstinline{C}, as well, but it did not result in a performance increase. 

\subsubsection{Memory Alignment}
The buffers for \lstinline{A} and \lstinline{B} are 16-byte aligned. This is important for vectorization, because it allows for aligned loads that are considerably faster than unaligned loads. In case the matrix size $n$ is odd, we add a $8$ byte padding at the end of each row. Therefore, every row is $16$-byte aligned\footnote{We are reading both \lstinline{A} and the transposed version of \lstinline{B} row by row.}.

\subsubsection{TLB Cache}
In general, matrix buffering is sometimes used to minimize TLB cache misses. The copied part of the matrix is often smaller and fits in less memory pages. We chose the block sizes for L1 blocking in such a way that every block of \lstinline{A}, \lstinline{B}, and \lstinline{C} fits entirely into the L1 cache. The CPU that we optimized for has a primary (first-level) TLB cache of size $64$ and a secondary (second-level) TLB cache of size $512$. Since the L1 cache can hold at most $\frac{32 \cdot 1024}{64} = 512$ cache lines, the TLB cache is big enough to cover a block of all three matrices at the same time.

\subsubsection{Performance}
\begin{figure}
	\includegraphics[width=\textwidth]{graphs/profiles/PROFILE_NO_BUFFER_A.pdf}
	\caption{Performance of our parameter-tuned blocking version, with and without buffering \lstinline{A}.}
	\label{fig:bufferA}
\end{figure}

Figure~\ref{fig:bufferA} compares the performance of our fully-optimized implementation with and without buffering \lstinline{A}. Note, that for the unbuffered version, we had to use unaligned loads for \lstinline{A} in the vectorized code, because the matrix size $n$ could be odd.

\subsection{Streaming SIMD Extensions (SSE) and Loop Unrolling}
SSE provides a set of vector registers that can contain multiple numbers at a time and support arithmetic operations on the whole vector at the same time. We tried and evaluted several implementations with a different number of vector registers and different levels of loop unrolling.

\subsubsection{Unrolling \lstinline{k += 2}}
Our first approach was to vectorize the innermost \lstinline{for} loop and unroll parts of the loop for \lstinline{k += 2}.

\begin{figure}[H]
\begin{lstlisting}[mathescape]
void do_block_unrolled_k (int lda, double* A, double* B, double* C)
{
	for (int i = 0; i < $b_i$; ++i)
	{
		for (int j = 0; j < $b_j$; ++j)
		{
			__m128d cij = _mm_setzero_pd();

			for (int k = 0; k < $b_k$; k += 2)
			{
				__m128d a =  _mm_load_pd(A + i*lda + k);
				__m128d b =  _mm_load_pd(B + j*lda + k);
				cij = _mm_add_pd(cij, _mm_mul_pd(a, b)); 
			}

			C[i*lda + j] += _mm_cvtsd_f64(cij) + _mm_cvtsd_f64(_mm_unpackhi_pd(cij, cij));
		}
	}
}
\end{lstlisting}
\caption{Basic block function with unrolling for \lstinline{k += 2}.}
\label{fig:basic_sse}
\end{figure}

This leads to a significant performance increase, for the following reasons.
\begin{itemize}
	\item SSE multiplications and additions operate on two doubles at the same time. Therefore, we can in theory process twice as many data at the same time.
	\item In line~13, the processor can multiply and add at the same time, because it has fused multiply-add unit.
	\item The loop in line~9 is executed fewer times, because we increased the step size. Therefore, we have less branches in the assembly code and less additions of the loop variable \lstinline{k}.
\end{itemize}

We tried increasing the step size and loading more values of \lstinline{A} and \lstinline{B} at the same time, in order to have less branches in the assembly code and to take advantage of more vector registers. In the previous code we used only $3$, maybe $4$\footnote{Depending on how the compiler translates line~16.}, registers, but the CPU has $16$ vector registers. Increasing the step size to $4$ or $8$ had, however, only a small effect on the performance. Unrolling \lstinline{i} and \lstinline{j} instead, yields a much greater performance increase.

\subsubsection{Unrolling \lstinline{j += 2, k += 2}}
We could increase the performance by unrolling both \lstinline{j += 2} and \lstinline{k += 2} at the same time.

\begin{figure}[H]
\begin{lstlisting}[mathescape]
void do_block_unrolled_k (int lda, double* A, double* B, double* C)
{
	for (int i = 0; i < $b_i$; ++i)
	{
		for (int j = 0; j < $b_j$; j += 2)
		{
			__m128d cij_1 = _mm_setzero_pd();
			__m128d cij_2 = _mm_setzero_pd();

			for (int k = 0; k < $b_k$; k += 2)
			{
				__m128d a =  _mm_load_pd(A + i*lda + k);
				__m128d b_1 =  _mm_load_pd(B + j*lda + k);
				__m128d b_2 =  _mm_load_pd(B + (j+1)*lda + k);
				cij_1 = _mm_add_pd(cij_1, _mm_mul_pd(a, b_1)); 
				cij_2 = _mm_add_pd(cij_2, _mm_mul_pd(a, b_2)); 
			}

			C[i*lda + j] += _mm_cvtsd_f64(cij_1) + _mm_cvtsd_f64(_mm_unpackhi_pd(cij_1, cij_1));
			C[i*lda + j + 1] += _mm_cvtsd_f64(cij_2) + _mm_cvtsd_f64(_mm_unpackhi_pd(cij_2, cij_2));
		}
	}
}
\end{lstlisting}
\caption{Optimized block function with unrolling for \lstinline{j += 2, k += 2}.}
\label{fig:opt_1_sse}
\end{figure}

Note, that in comparison to Figure~\ref{fig:basic_sse}, the value \lstinline{a} is read only once instead of twice. That is the main reason why this code is faster. Besides, we save some branches and arithmetic operations due to the increased step size of \lstinline{j}. In total, we use $5$, maybe $7$, vector registers. This suggests that it may be useful to do more loop unrolling. Unrolling for \lstinline{j += 4, k += 2} or \lstinline{j += 2, k += 4} had only little effect on the performance.

\subsubsection{Unrolling \lstinline{i += 2, j += 2, k += 2}}
The code for unrolling all three loops adds another load of \lstinline{A + (i+1)*lda + k} to the code in Figure~\ref{fig:opt_1_sse}. Besides, we need two more vector registers \lstinline{cij_3} and \lstinline{cij_4}, because we accumulate values for four different positions in \lstinline{C}. This optimization incrases the performance, because \lstinline{b_1} and \lstinline{b_2} are loaded only once, for two different values \lstinline{a_1} and \lstinline{a_2}. We use at least $8$ vector registers at this point.

\subsubsection{Unrolling \lstinline{i += 2, j += 2, k += 4}}
This is what we did in the final version of our implementation. Increasing the step size of \lstinline{k} does not save any loads but reduces the number of branches and uses more vector registers: $4$ registers \lstinline{cij_*} to accumulate the increment of $c_{ij}$, $8$ vector registers for loading values of \lstinline{A} and \lstinline{B}, and probably some vector registers for storing intermediate results for \lstinline{_mm_unpackhi_pd}. By unrolling the loops even further, we can not take advantage of additional vector registers, because the CPU has only $16$ vector registers.

\subsubsection{Performance}
\begin{figure}
	\includegraphics[width=\textwidth]{graphs/profiles/PROFILE_UNROLLING_SSE.pdf}
	\caption{Performance of our parameter-tuned blocking version, with different levels of loop unrolling.}
	\label{fig:sse_opt_graph}
\end{figure}

Figure~\ref{fig:sse_opt_graph} shows that, in most cases, unrolling for \lstinline{i += 2, j += 2, k += 4} performs best. Therefore, we use this version in our implementation.

\subsection{Constant Propagation and Code Duplication}
% duplicate code with constants instead of variable access

\subsection{Parameter Tuning}
Parameter tuning is the task of trying (brute-force) a set of parameters that affect the performance.

\subsubsection{Motivation}
As shown before, for certain matrix sizes $n$, the blocked algorithm performes better with certain block sizes. One obvious reason is that for a fixed matrix size $n$, the block size determines the size of the fringe cases. The code for fringe cases is less optimized than the code for regular cases and fringe cases might take less advantage of the cache. Also remember, that a cache line is $64$ bytes long and the size of a fringe case might not be divisible by the cache line size, i.e. a cache line is eventually not filled entirely with data from the fringe case. Therefore, we want to keep the fringe cases as small as possible. Let $f_i$ be the size of the fringe cases in dimension $i$\footnote{The same argument applies for the other dimensions.}. 

$$f_i = n \mod b_i$$

For example, for $n=372$, $b_i=16$ might be a better block size than $b_i=32$, because it results in fringe cases of size $4$ instead of $20$.

\subsubsection{Search Space}
We tried values for $b_i$, $b_j$ and $b_k$ for the L1 cache blocking. Due to certain implementation details, $b_i$ and $b_j$ must always be the same value. A cache line is $64$ bytes ($8$ doubles) long. Therefore, in order to reduce the search space, it makes sense to only take a look at multiples of $8$. We tried all possible combinations of $b_i=b_j, b_k \in \{8, 16, 24, 32, 40, 48, 56, 64\}$, resulting in $64$ parameter combinations. We ran the program for every matrix size from $n=10$ to $n=1200$. For every value of $n$, we then selected the parameters $b_i$ and $b_k$ with the highest performance. 

\subsubsection{Implementation}
We hard-coded the block size parameters for all matrix sizes from $n=10$ to $n=1200$. Based on the matrix size, a different version of the algorithm with the respective block size is run. For all other matrix sizes, we use $b_i=b_j=8$, $b_k=64$, because it performed good on average.

\subsubsection{Results and Evaluation}
Figure~\ref{fig:param_results} shows the performance of the parameter-tuned version of our implementation. It is interesting to see, that for some very small matrix sizes, our implementation is faster than ATLAS. The Appendix contains some more graphs without parameter tuning, but for fixed block sizes. We can observe that the performance of the algorithm reaches a peak when the matrix size is a multiple of one or two of block sizes. In that case, we have no fringe cases.
\begin{figure}
	\includegraphics[width=\textwidth]{graphs/profiles/PROFILE_BLOCKED.pdf}
	\caption{Performance of our parameter-tuned blocking version, in comparison to ATLAS.}
	\label{fig:param_results}
\end{figure}

\newpage
\section*{Appendix}
\subsection*{Performance Tuning}
%\begin{tabular}{cccc}
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_8_8.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_8_16.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_8_32.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_8_64.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_16_8.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_16_16.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_32.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_64_64.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_8.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_16.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_32.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_64.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_64_8.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_64_16.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_64_32.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_64_64.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_24_56.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_56.pdf}  
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_32_56.pdf} 
	\includegraphics[width=0.25\textwidth]{graphs/profiles/PROFILE_OUTUT_40_56.pdf} 
%\end{tabular}
\end{document}

